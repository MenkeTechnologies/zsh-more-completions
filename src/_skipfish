#compdef skipfish

local arguments

arguments=(
  '-A[use specified HTTP authentication credentials]'
  '-F[IP - pretend that host resolves to IP]'
  '-C[val - append a custom cookie to all requests]'
  '-H[val - append a custom HTTP header to all requests]'
  '-b[) - use headers consistent with MSIE / Firefox / iPhone]'
  '-N[do not accept any new cookies]'
  '--auth-form[form authentication URL]'
  '--auth-user[form authentication user]'
  '--auth-pass[form authentication password]'
  '--auth-verify-url[ URL for in-session detection]'
  '-d[maximum crawl tree depth (16)]'
  '-c[maximum children to index per node (512)]'
  '-x[maximum descendants to index per branch (8192)]'
  '-r[max total number of requests to send (100000000)]'
  '-p[% - node and link crawl probability (100%)]'
  '-q[repeat probabilistic scan with given seed]'
  '-I[only follow URLs matching string]'
  '-X[exclude URLs matching string]'
  '-K[do not fuzz parameters named string]'
  '-D[crawl cross-site links to another domain]'
  '-B[trust, but do not crawl, another domain]'
  '-Z[do not descend into 5xx locations]'
  '-O[do not submit any forms]'
  '-P[do not parse HTML, etc, to find new links]'
  '-o[write output to specified directory (required)]'
  '-M[log warnings about mixed content / non-SSL passwords]'
  '-E[log all HTTP/1.0 / HTTP/1.1 caching intent mismatches]'
  '-U[log all external URLs and e-mails seen]'
  '-Q[completely suppress duplicate nodes in reports]'
  '-u[be quiet, disable realtime progress stats]'
  '-v[enable runtime logging (to stderr)]'
  '-W[use a specified read-write wordlist (required)]'
  '-S[load a supplemental read-only wordlist]'
  '-L[do not auto-learn new keywords for the site]'
  '-Y[do not fuzz extensions in directory brute-force]'
  '-R[purge words hit more than age scans ago]'
  '-T[val - add new form auto-fill rule]'
  '-G[maximum number of keyword guesses to keep (256)]'
  '-z[load signatures from this file]'
  '-g[max simultaneous TCP connections, global (40)]'
  '-m[max simultaneous connections, per target IP (10)]'
  '-f[max number of consecutive HTTP errors (100)]'
  '-t[total request response timeout (20 s)]'
  '-w[individual network I/O timeout (10 s)]'
  '-i[timeout on idle HTTP connections (10 s)]'
  '-s[response size limit (400000 B)]'
  '-e[do not keep binary responses for reporting]'
  '-l[max requests per second (0.000000)]'
  '-k[stop scanning after the given duration h:m:s]'
  '--config[load the specified configuration file]'
  '*:filename:_files'
)

_arguments -s $arguments
